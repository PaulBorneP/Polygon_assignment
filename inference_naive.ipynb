{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from efficientnet_pytorch import EfficientNet\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "import os\n",
    "from tqdm import *\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import numpy as np\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained weights for efficientnet-b0\n"
     ]
    }
   ],
   "source": [
    "from efficientnet_pytorch import EfficientNet\n",
    "model = EfficientNet.from_pretrained('efficientnet-b0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference on a single image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shetland sheepdog, Shetland sheep dog, Shetland                             (66.55%)\n",
      "collie                                                                      (26.09%)\n",
      "borzoi, Russian wolfhound                                                   (0.27%)\n",
      "Pembroke, Pembroke Welsh corgi                                              (0.13%)\n",
      "papillon                                                                    (0.12%)\n"
     ]
    }
   ],
   "source": [
    "# Preprocess image\n",
    "tfms = transforms.Compose([transforms.Resize(224), transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),])\n",
    "test_image = tfms(Image.open('./dataset/ILSVRC2012_val_00000003.JPEG')).unsqueeze(0)\n",
    "\n",
    "# Load ImageNet class names\n",
    "labels_map = json.load(open('categories.json'))\n",
    "labels_map = [labels_map[str(i)] for i in range(1000)]\n",
    "\n",
    "# Classify\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = model(test_image)\n",
    "\n",
    "# Print predictions\n",
    "for idx in torch.topk(outputs, k=5).indices.squeeze(0).tolist():\n",
    "    prob = torch.softmax(outputs, dim=1)[0, idx].item()\n",
    "    print('{label:<75} ({p:.2f}%)'.format(label=labels_map[idx], p=prob*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive implementation on the whole dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:43<00:00,  4.60it/s]\n"
     ]
    }
   ],
   "source": [
    "directory= './dataset/'\n",
    "nb_samples=200 # test on a smaller part of the dataset\n",
    "labels=json.load(open('labels.json'))\n",
    "\n",
    "grayscale= []\n",
    "actual=[]\n",
    "predicted=[]\n",
    "\n",
    "tfms = transforms.Compose([transforms.Resize(224), transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),])\n",
    "\n",
    "for filename in tqdm(os.listdir(directory)[:nb_samples]):\n",
    "    f = os.path.join(directory, filename)\n",
    "    y_true= labels[filename]    \n",
    "    img=Image.open(f)    \n",
    "    if img.mode !='RGB':\n",
    "        grayscale.append(filename)\n",
    "        img = tfms(img.convert('RGB')).unsqueeze(0)\n",
    "    else:\n",
    "        img = tfms(img).unsqueeze(0)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(img)\n",
    "        y_pred = torch.argmax(outputs).item()\n",
    "        actual.append(y_true)\n",
    "        predicted.append(y_pred)\n",
    "\n",
    "with open('results.json', 'w') as fp:\n",
    "    results = { i : (actual[i], predicted[i]) for i in range(nb_samples)}\n",
    "    json.dump(results, fp,  indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[511, 74, 32, 332, 928, 346, 10, 259, 56, 524, 841, 618, 31, 803, 566, 990, 964, 52, 7, 547, 52, 444, 741, 72, 113, 504, 817, 462, 205, 425, 215, 871, 203, 135, 837, 491, 703, 563, 793, 83, 898, 31, 7, 962, 354, 128, 424, 463, 438, 541, 359, 973, 985, 816, 346, 723, 148, 580, 449, 510, 486, 237, 524, 791, 979, 593, 939, 913, 451, 627, 763, 54, 401, 553, 982, 41, 135, 707, 170, 48, 180, 204, 482, 212, 674, 990, 164, 701, 519, 23, 543, 736, 264, 278, 438, 735, 649, 10, 410, 91, 92, 355, 318, 120, 596, 956, 864, 679, 42, 762, 806, 136, 690, 357, 371, 543, 414, 935, 581, 849, 30, 845, 513, 490, 350, 858, 787, 114, 14, 96, 620, 988, 48, 848, 626, 913, 209, 717, 121, 250, 896, 380, 297, 626, 395, 180, 628, 493, 551, 795, 158, 819, 356, 70, 639, 661, 817, 321, 78, 967, 939, 168, 285, 270, 448, 366, 775, 515, 53, 413, 3, 20, 928, 490, 100, 918, 597, 119, 754, 452, 886, 321, 253, 942, 417, 17, 824, 858, 800, 926, 388, 280, 666, 115, 22, 717, 607, 859, 270, 188]\n"
     ]
    }
   ],
   "source": [
    "results=list(json.load(open('results.json')).values())\n",
    "actual=[sample[0] for sample in results]\n",
    "predicted=[sample[1] for sample in results]\n",
    "\n",
    "print(actual)\n",
    "def confusion_values (y_true,y_pred):\n",
    "    confusion = confusion_matrix(y_true,y_pred)\n",
    "    FP = confusion.sum(axis=0) - np.diag(confusion)  \n",
    "    FN = confusion.sum(axis=1) - np.diag(confusion)\n",
    "    TP = np.diag(confusion)\n",
    "    TN = confusion.sum() - (FP + FN + TP)\n",
    "    return FP,FN,TP,TN\n",
    "\n",
    "def get_metrics(FP,FN,TP,TN,nb_smpl):\n",
    "    # Accuracy\n",
    "    ACC=sum(TP)/nb_smpl\n",
    "    # Specificity or True Negative Rate\n",
    "    TNR=TN/(TN+FP)\n",
    "    return {'ACC':ACC,'TNR':TNR}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Global metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ACC': 0.745, 'TNR': array([1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       0.99497487, 0.995     , 0.995     , 1.        , 1.        ,\n",
      "       0.995     , 0.99494949, 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 0.995     , 1.        ,\n",
      "       1.        , 1.        , 0.995     , 1.        , 1.        ,\n",
      "       1.        , 1.        , 0.995     , 1.        , 1.        ,\n",
      "       1.        , 0.995     , 0.99497487, 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 0.995     ,\n",
      "       0.995     , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 0.995     , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 0.995     ,\n",
      "       1.        , 0.995     , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 0.99497487, 0.995     ,\n",
      "       1.        , 0.995     , 1.        , 0.995     , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 0.99497487, 0.995     , 1.        ,\n",
      "       1.        , 1.        , 1.        , 0.995     , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 0.995     , 1.        , 1.        , 1.        ,\n",
      "       0.995     , 0.995     , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 0.995     , 0.99      , 1.        ,\n",
      "       1.        , 0.99      , 0.995     , 0.99497487, 0.995     ,\n",
      "       1.        , 1.        , 1.        , 1.        , 0.995     ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 0.995     ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 0.995     , 1.        , 1.        ,\n",
      "       0.995     , 1.        , 1.        , 1.        , 0.995     ,\n",
      "       0.995     , 1.        , 0.995     , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 0.99497487, 1.        , 0.995     ,\n",
      "       1.        , 0.995     , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 0.995     ,\n",
      "       1.        , 1.        , 1.        , 0.995     , 1.        ,\n",
      "       1.        , 1.        , 0.99497487, 0.99497487, 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 0.995     ,\n",
      "       1.        , 1.        , 1.        , 1.        , 0.995     ,\n",
      "       1.        , 1.        , 0.995     , 1.        , 1.        ,\n",
      "       1.        , 0.99494949])}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           3       1.00      1.00      1.00         1\n",
      "           7       1.00      1.00      1.00         2\n",
      "          10       1.00      1.00      1.00         2\n",
      "          14       1.00      1.00      1.00         1\n",
      "          17       1.00      1.00      1.00         1\n",
      "          20       1.00      1.00      1.00         1\n",
      "          22       1.00      1.00      1.00         1\n",
      "          23       1.00      1.00      1.00         1\n",
      "          30       0.00      0.00      0.00         1\n",
      "          31       1.00      1.00      1.00         2\n",
      "          32       0.00      0.00      0.00         1\n",
      "          33       0.00      0.00      0.00         0\n",
      "          39       0.00      0.00      0.00         0\n",
      "          41       0.00      0.00      0.00         1\n",
      "          42       0.00      0.00      0.00         1\n",
      "          46       0.00      0.00      0.00         0\n",
      "          48       0.50      0.50      0.50         2\n",
      "          52       1.00      1.00      1.00         2\n",
      "          53       1.00      1.00      1.00         1\n",
      "          54       1.00      1.00      1.00         1\n",
      "          56       1.00      1.00      1.00         1\n",
      "          70       1.00      1.00      1.00         1\n",
      "          72       1.00      1.00      1.00         1\n",
      "          73       0.00      0.00      0.00         0\n",
      "          74       1.00      1.00      1.00         1\n",
      "          78       0.00      0.00      0.00         1\n",
      "          83       1.00      1.00      1.00         1\n",
      "          86       0.00      0.00      0.00         0\n",
      "          91       0.00      0.00      0.00         1\n",
      "          92       1.00      1.00      1.00         1\n",
      "          96       1.00      1.00      1.00         1\n",
      "         100       1.00      1.00      1.00         1\n",
      "         110       0.00      0.00      0.00         0\n",
      "         113       0.00      0.00      0.00         1\n",
      "         114       1.00      1.00      1.00         1\n",
      "         115       0.00      0.00      0.00         1\n",
      "         118       0.00      0.00      0.00         0\n",
      "         119       0.00      0.00      0.00         1\n",
      "         120       0.00      0.00      0.00         1\n",
      "         121       0.00      0.00      0.00         1\n",
      "         128       1.00      1.00      1.00         1\n",
      "         135       1.00      1.00      1.00         2\n",
      "         136       1.00      1.00      1.00         1\n",
      "         148       1.00      1.00      1.00         1\n",
      "         151       0.00      0.00      0.00         0\n",
      "         153       0.00      0.00      0.00         0\n",
      "         158       0.00      0.00      0.00         1\n",
      "         164       1.00      1.00      1.00         1\n",
      "         168       1.00      1.00      1.00         1\n",
      "         170       1.00      1.00      1.00         1\n",
      "         180       1.00      0.50      0.67         2\n",
      "         188       1.00      1.00      1.00         1\n",
      "         202       0.00      0.00      0.00         0\n",
      "         203       0.00      0.00      0.00         1\n",
      "         204       1.00      1.00      1.00         1\n",
      "         205       1.00      1.00      1.00         1\n",
      "         209       1.00      1.00      1.00         1\n",
      "         212       1.00      1.00      1.00         1\n",
      "         215       1.00      1.00      1.00         1\n",
      "         225       0.00      0.00      0.00         0\n",
      "         237       1.00      1.00      1.00         1\n",
      "         248       0.00      0.00      0.00         0\n",
      "         250       0.00      0.00      0.00         1\n",
      "         253       1.00      1.00      1.00         1\n",
      "         259       1.00      1.00      1.00         1\n",
      "         264       1.00      1.00      1.00         1\n",
      "         270       1.00      1.00      1.00         2\n",
      "         278       0.00      0.00      0.00         1\n",
      "         280       0.50      1.00      0.67         1\n",
      "         281       0.00      0.00      0.00         0\n",
      "         285       0.00      0.00      0.00         1\n",
      "         294       0.00      0.00      0.00         0\n",
      "         297       1.00      1.00      1.00         1\n",
      "         313       0.00      0.00      0.00         0\n",
      "         318       1.00      1.00      1.00         1\n",
      "         321       1.00      1.00      1.00         2\n",
      "         332       0.00      0.00      0.00         1\n",
      "         346       1.00      1.00      1.00         2\n",
      "         350       1.00      1.00      1.00         1\n",
      "         354       1.00      1.00      1.00         1\n",
      "         355       1.00      1.00      1.00         1\n",
      "         356       0.00      0.00      0.00         1\n",
      "         357       0.50      1.00      0.67         1\n",
      "         358       0.00      0.00      0.00         0\n",
      "         359       0.00      0.00      0.00         1\n",
      "         366       1.00      1.00      1.00         1\n",
      "         371       1.00      1.00      1.00         1\n",
      "         380       0.00      0.00      0.00         1\n",
      "         382       0.00      0.00      0.00         0\n",
      "         388       1.00      1.00      1.00         1\n",
      "         395       1.00      1.00      1.00         1\n",
      "         401       1.00      1.00      1.00         1\n",
      "         410       1.00      1.00      1.00         1\n",
      "         413       0.00      0.00      0.00         1\n",
      "         414       1.00      1.00      1.00         1\n",
      "         417       1.00      1.00      1.00         1\n",
      "         422       0.00      0.00      0.00         0\n",
      "         424       0.00      0.00      0.00         1\n",
      "         425       1.00      1.00      1.00         1\n",
      "         438       0.00      0.00      0.00         2\n",
      "         441       0.00      0.00      0.00         0\n",
      "         442       0.00      0.00      0.00         0\n",
      "         444       1.00      1.00      1.00         1\n",
      "         448       1.00      1.00      1.00         1\n",
      "         449       1.00      1.00      1.00         1\n",
      "         451       1.00      1.00      1.00         1\n",
      "         452       1.00      1.00      1.00         1\n",
      "         455       0.00      0.00      0.00         0\n",
      "         461       0.00      0.00      0.00         0\n",
      "         462       1.00      1.00      1.00         1\n",
      "         463       0.00      0.00      0.00         1\n",
      "         469       0.00      0.00      0.00         0\n",
      "         480       0.00      0.00      0.00         0\n",
      "         482       0.00      0.00      0.00         1\n",
      "         485       0.00      0.00      0.00         0\n",
      "         486       1.00      1.00      1.00         1\n",
      "         490       1.00      1.00      1.00         2\n",
      "         491       1.00      1.00      1.00         1\n",
      "         493       1.00      1.00      1.00         1\n",
      "         497       0.00      0.00      0.00         0\n",
      "         504       1.00      1.00      1.00         1\n",
      "         510       1.00      1.00      1.00         1\n",
      "         511       1.00      1.00      1.00         1\n",
      "         513       1.00      1.00      1.00         1\n",
      "         515       1.00      1.00      1.00         1\n",
      "         519       0.00      0.00      0.00         1\n",
      "         524       1.00      0.50      0.67         2\n",
      "         541       0.00      0.00      0.00         1\n",
      "         543       1.00      0.50      0.67         2\n",
      "         547       1.00      1.00      1.00         1\n",
      "         551       0.00      0.00      0.00         1\n",
      "         553       1.00      1.00      1.00         1\n",
      "         563       1.00      1.00      1.00         1\n",
      "         566       1.00      1.00      1.00         1\n",
      "         575       0.00      0.00      0.00         0\n",
      "         580       0.00      0.00      0.00         1\n",
      "         581       1.00      1.00      1.00         1\n",
      "         593       0.00      0.00      0.00         1\n",
      "         596       1.00      1.00      1.00         1\n",
      "         597       0.00      0.00      0.00         1\n",
      "         607       1.00      1.00      1.00         1\n",
      "         618       0.00      0.00      0.00         1\n",
      "         620       1.00      1.00      1.00         1\n",
      "         626       1.00      1.00      1.00         2\n",
      "         627       1.00      1.00      1.00         1\n",
      "         628       1.00      1.00      1.00         1\n",
      "         639       1.00      1.00      1.00         1\n",
      "         648       0.00      0.00      0.00         0\n",
      "         649       1.00      1.00      1.00         1\n",
      "         661       1.00      1.00      1.00         1\n",
      "         665       0.00      0.00      0.00         0\n",
      "         666       1.00      1.00      1.00         1\n",
      "         674       1.00      1.00      1.00         1\n",
      "         679       1.00      1.00      1.00         1\n",
      "         684       0.00      0.00      0.00         0\n",
      "         686       0.00      0.00      0.00         0\n",
      "         690       1.00      1.00      1.00         1\n",
      "         691       0.00      0.00      0.00         0\n",
      "         701       1.00      1.00      1.00         1\n",
      "         703       1.00      1.00      1.00         1\n",
      "         707       0.00      0.00      0.00         1\n",
      "         717       1.00      1.00      1.00         2\n",
      "         723       1.00      1.00      1.00         1\n",
      "         735       1.00      1.00      1.00         1\n",
      "         736       1.00      1.00      1.00         1\n",
      "         741       1.00      1.00      1.00         1\n",
      "         754       0.00      0.00      0.00         1\n",
      "         762       0.50      1.00      0.67         1\n",
      "         763       1.00      1.00      1.00         1\n",
      "         764       0.00      0.00      0.00         0\n",
      "         775       0.00      0.00      0.00         1\n",
      "         785       0.00      0.00      0.00         0\n",
      "         787       0.00      0.00      0.00         1\n",
      "         791       1.00      1.00      1.00         1\n",
      "         793       1.00      1.00      1.00         1\n",
      "         795       1.00      1.00      1.00         1\n",
      "         800       1.00      1.00      1.00         1\n",
      "         803       1.00      1.00      1.00         1\n",
      "         806       0.00      0.00      0.00         1\n",
      "         807       0.00      0.00      0.00         0\n",
      "         816       1.00      1.00      1.00         1\n",
      "         817       1.00      0.50      0.67         2\n",
      "         819       1.00      1.00      1.00         1\n",
      "         821       0.00      0.00      0.00         0\n",
      "         824       1.00      1.00      1.00         1\n",
      "         837       0.00      0.00      0.00         1\n",
      "         841       1.00      1.00      1.00         1\n",
      "         845       0.00      0.00      0.00         1\n",
      "         848       0.00      0.00      0.00         1\n",
      "         849       1.00      1.00      1.00         1\n",
      "         858       0.00      0.00      0.00         2\n",
      "         859       1.00      1.00      1.00         1\n",
      "         864       1.00      1.00      1.00         1\n",
      "         871       1.00      1.00      1.00         1\n",
      "         886       1.00      1.00      1.00         1\n",
      "         896       1.00      1.00      1.00         1\n",
      "         898       1.00      1.00      1.00         1\n",
      "         913       1.00      1.00      1.00         2\n",
      "         918       1.00      1.00      1.00         1\n",
      "         926       1.00      1.00      1.00         1\n",
      "         928       1.00      1.00      1.00         2\n",
      "         935       1.00      1.00      1.00         1\n",
      "         939       1.00      0.50      0.67         2\n",
      "         942       1.00      1.00      1.00         1\n",
      "         943       0.00      0.00      0.00         0\n",
      "         956       1.00      1.00      1.00         1\n",
      "         962       1.00      1.00      1.00         1\n",
      "         964       0.00      0.00      0.00         1\n",
      "         967       1.00      1.00      1.00         1\n",
      "         972       0.00      0.00      0.00         0\n",
      "         973       1.00      1.00      1.00         1\n",
      "         979       0.00      0.00      0.00         1\n",
      "         981       0.00      0.00      0.00         0\n",
      "         982       1.00      1.00      1.00         1\n",
      "         985       1.00      1.00      1.00         1\n",
      "         988       1.00      1.00      1.00         1\n",
      "         990       0.67      1.00      0.80         2\n",
      "\n",
      "    accuracy                           0.74       200\n",
      "   macro avg       0.61      0.61      0.61       200\n",
      "weighted avg       0.76      0.74      0.75       200\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/paulbp_dty/Desktop/src/ml_assign/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/paulbp_dty/Desktop/src/ml_assign/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/paulbp_dty/Desktop/src/ml_assign/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/paulbp_dty/Desktop/src/ml_assign/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/paulbp_dty/Desktop/src/ml_assign/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/paulbp_dty/Desktop/src/ml_assign/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "FP,FN,TP,TN = confusion_values(actual,predicted)\n",
    "metrics = get_metrics(FP,FN,TP,TN,nb_samples)\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Class imbalance\n",
    "We count the frequence of appearance of each class in order to compute metrics on the different parts of the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frequence\n",
    "We compute the metrics on the most present (true) classes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_dict = {x:actual.count(x) for x in actual}\n",
    "predicted_dict = {x:predicted.count(x) for x in predicted}\n",
    "results_dict = {actual[i]: predicted[i] for i in range(len(actual))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ACC': 0.7658450704225352, 'TNR': array([1.        , 1.        , 1.        , 1.        , 0.99823944,\n",
      "       1.        , 0.99823944, 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 0.99823944,\n",
      "       1.        , 0.99823944, 1.        , 1.        , 1.        ,\n",
      "       0.99823944, 1.        , 1.        , 1.        , 1.        ,\n",
      "       0.99823944, 0.99823633, 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 0.99823944, 1.        , 1.        ,\n",
      "       0.99470899, 0.99823633, 1.        , 1.        , 1.        ,\n",
      "       1.        , 0.99823633, 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 0.99823944, 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 0.99823633,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 0.99823633, 0.99823633, 1.        , 1.        ,\n",
      "       1.        , 0.99823944, 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 0.99823944, 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 0.99823633,\n",
      "       0.99823944, 0.99823944, 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 0.99823633,\n",
      "       1.        , 1.        , 0.99823944, 1.        , 1.        ,\n",
      "       1.        , 0.99823944, 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       0.99823944, 0.99647887, 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 0.99823633, 1.        ,\n",
      "       0.99823633, 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 0.99823944, 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 0.99823633, 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       0.99647266, 1.        , 0.99647266, 1.        , 1.        ,\n",
      "       1.        , 1.        , 0.99823633, 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       0.99823633, 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 0.99823944, 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       0.99823633, 1.        , 1.        , 0.99823633, 1.        ,\n",
      "       1.        , 1.        , 0.99823944, 1.        , 1.        ,\n",
      "       1.        , 0.99823944, 1.        , 0.99823633, 1.        ,\n",
      "       1.        , 1.        , 1.        , 0.99823633, 0.99823633,\n",
      "       1.        , 1.        , 0.99647266, 0.99823633, 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 0.99823944, 1.        , 1.        , 0.99823944,\n",
      "       1.        , 0.99823633, 1.        , 1.        , 1.        ,\n",
      "       1.        , 0.99823633, 1.        , 0.99823944, 0.99823944,\n",
      "       1.        , 1.        , 1.        , 0.99823944, 1.        ,\n",
      "       1.        , 0.99823633, 0.99823944, 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 0.99823633, 0.99823633, 1.        , 0.99823633,\n",
      "       0.99823944, 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       0.99823633, 1.        , 0.99823633, 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 0.99823944, 1.        , 1.        , 1.        ,\n",
      "       1.        , 0.99823944, 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 0.99823633, 1.        ,\n",
      "       1.        , 0.99647266, 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       0.99823944, 0.99823633, 1.        , 1.        , 1.        ,\n",
      "       0.99823633, 1.        , 0.99823944, 1.        , 1.        ,\n",
      "       1.        , 1.        , 0.99823944, 1.        , 1.        ,\n",
      "       1.        , 0.99823633, 1.        , 0.99823633, 1.        ,\n",
      "       1.        , 1.        , 0.99823944, 1.        , 1.        ,\n",
      "       1.        , 1.        , 0.99823944, 1.        , 1.        ,\n",
      "       0.99823944, 1.        , 0.99823944, 0.99823944, 1.        ,\n",
      "       1.        , 1.        , 1.        , 0.99823633, 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 0.99823944,\n",
      "       1.        , 1.        , 0.99823633, 1.        , 1.        ,\n",
      "       0.99823944, 1.        , 1.        , 0.99647887, 0.99823633,\n",
      "       1.        , 1.        , 1.        , 0.99823633, 1.        ,\n",
      "       1.        , 0.99823633, 1.        , 1.        , 1.        ,\n",
      "       0.99823633, 1.        , 1.        , 0.99823944, 1.        ,\n",
      "       1.        , 1.        , 0.99647266, 1.        , 1.        ,\n",
      "       0.99823633, 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 0.99823633,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 0.99823633, 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       0.99823633, 1.        , 0.99647266, 1.        , 1.        ,\n",
      "       1.        , 1.        , 0.99647887, 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 0.99823633, 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 0.99823944, 1.        , 1.        ,\n",
      "       1.        , 1.        , 0.99823944, 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 0.99823633, 0.99823944,\n",
      "       1.        , 0.99823944, 1.        , 1.        , 1.        ,\n",
      "       1.        , 0.99823944, 0.99823633, 1.        , 1.        ,\n",
      "       1.        , 1.        , 0.99823944, 1.        , 1.        ,\n",
      "       0.99823633, 1.        , 0.99823944, 0.99823944, 1.        ,\n",
      "       0.99471831, 1.        , 1.        , 1.        , 0.99823944,\n",
      "       1.        , 1.        , 0.99823633, 1.        , 1.        ,\n",
      "       0.99823633, 1.        , 0.99823633, 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 0.99647887, 1.        ,\n",
      "       1.        , 0.99823633, 1.        , 0.99823944, 1.        ,\n",
      "       1.        , 1.        , 1.        , 0.99823944, 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 0.99823944,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 0.99823633,\n",
      "       0.99823944, 1.        , 1.        , 0.99823633, 1.        ,\n",
      "       1.        , 0.99823944, 1.        , 1.        , 0.99823633,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 0.99823944, 1.        ,\n",
      "       1.        , 1.        ])}\n"
     ]
    }
   ],
   "source": [
    "actual_filtered = [ key for (key,value) in actual_dict.items() if value >= 5 ]\n",
    "results_filtered = {key:value for (key,value) in results_dict.items() if key in actual_filtered }\n",
    "fFP,fFN,fTP,fTN = confusion_values(list(results_filtered.keys()),list(results_filtered.values()))\n",
    "fmetrics = get_metrics(fFP,fFN,fTP,fTN,len(actual_filtered))\n",
    "print(fmetrics.ACC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grayscale\n",
    "We compute the metrics on the grayscale images to see if the color have an effect on the inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ACC': 0.63, 'TNR': array([1.        , 1.        , 0.98947368, 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 0.9893617 ,\n",
      "       1.        , 1.        , 0.98947368, 0.98947368, 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 0.98947368, 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 0.98947368,\n",
      "       1.        , 1.        , 0.98947368, 1.        , 0.98947368,\n",
      "       1.        , 0.9893617 , 1.        , 1.        , 0.98947368,\n",
      "       1.        , 1.        , 0.98947368, 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 0.98947368, 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 0.98947368, 1.        , 0.9893617 , 1.        ,\n",
      "       0.97894737, 1.        , 1.        , 1.        , 0.98947368,\n",
      "       0.98947368, 0.98947368, 0.98947368, 1.        , 0.98947368,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 0.98947368, 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       0.98947368, 0.98947368, 1.        , 1.        , 1.        ,\n",
      "       0.98947368, 1.        , 0.98947368, 1.        , 1.        ,\n",
      "       0.98947368, 1.        , 0.98947368, 1.        , 1.        ,\n",
      "       0.98947368, 1.        , 1.        , 0.9893617 , 1.        ,\n",
      "       1.        , 1.        , 0.98947368, 1.        , 1.        ,\n",
      "       0.98947368, 1.        ])}\n"
     ]
    }
   ],
   "source": [
    "grayscale_labels = [labels[filename] for filename in grayscale  ]\n",
    "results_gray = {key:value for (key,value) in results_dict.items() if key in grayscale_labels }\n",
    "gFP,gFN,gTP,gTN = confusion_values(list(results_gray.keys()),list(results_gray.values()))\n",
    "gmetrics = get_metrics(gFP,gFN,gTP,gTN,len(grayscale_labels))\n",
    "print(gmetrics.ACC)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ea40ca8dbc11a9410171efd567f9969a243e602de40adcaaf631b75071b2ebce"
  },
  "kernelspec": {
   "display_name": "Python 3.10.2 ('ml_assign': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
